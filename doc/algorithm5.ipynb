{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prejudice\n",
    "\n",
    "**Prejudice** means a statistical dependence between a sensitive variable,\n",
    "$S$, and the target variable, $Y$, or a non-sensitive variable, $X$.\n",
    "\n",
    "There are three types of prejudices:\n",
    "## Direct prejudice\n",
    "\n",
    "Direct prejudice is the use of a sensitive variable in a prediction model.\n",
    "\n",
    "To eliminate direct prejudice, we can remove the sensitive variable from the model.\n",
    "\n",
    "\n",
    "## Indirect prejudice\n",
    "\n",
    "Indirect prejudice is statistical dependence between a sensitive variable and a target variable.\n",
    "\n",
    "To remove this indirect prejudice, we must use a prediction model that satisfies the condition $Y \\perp\\!\\!\\!\\perp \\ S$.\n",
    "\n",
    "We can quantify the degree of indirect prejudice using the following equation where $PI$ refers to the (indirect) prejudice index and $\\cal{D}$ is the data set.\n",
    "\n",
    "$$\\text{PI} = \\sum_{(y, s) \\in \\cal{D}}  \\hat{\\text{Pr}}[y, s] \\ln \\frac{\\hat{\\text{Pr}}[y, s]}{\\hat{\\text{Pr}}[y]\\hat{\\text{Pr}}[s]}$$\n",
    "\n",
    "The application of the normalization technique for mutual information leads to a _normalized prejudice index_ (NPI)\n",
    "\n",
    "$$\\text{NPI} = \\frac{\\text{PI}}{\\sqrt{\\text{H}(Y)\\text{H}(S)}}$$\n",
    "\n",
    "where $\\text{H}(\\ \\cdot\\ )$ is the entropy function.\n",
    "\n",
    "## Latent prejudice\n",
    "Latent prejudice is a statistical dependence between a sensitive variable, $S$, and a non-sensitive variable, $X$.\n",
    "\n",
    "Removal of potential prejudice is achieved by making $X$ and $Y$ independent from $S$ simultaneously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underestimation\n",
    "\n",
    "Underestimation is the state in which a learned model is not fully converged due to the finiteness of the size of a training data set.\n",
    "\n",
    "Despite that a prediction model without indirect prejudice can learn to make a fair determination, this is only the case if we have an \"infinitely large\" training data set. In general, training sets are finite and limited to small quantities of data, hence the model could output even more unfair determinations than that observed in the training sample distribution.\n",
    "\n",
    "To quantify the degree of underestimation, we assess the resultant difference between the training sample distribution over $\\cal{D}$, $\\tilde{\\text{Pr}}$ using the underestimation index (UEI) which is calculated using the Hellinger distance:\n",
    "\n",
    "$$\\text{UEI} = \\sqrt{\\frac{1}{2}\\sum_{(y, s) \\in \\cal{D}} \\left(\\sqrt{\\tilde{\\text{Pr}}[y, s]} - \\sqrt{\\hat{\\text{Pr}}[y, s]}\\right)^2} = \\sqrt{1 - \\sum_{(y, s) \\in \\cal{D}} \\sqrt{\\hat{\\text{Pr}}[Y, S]\\tilde{\\text{Pr}}[Y, S]}}$$\n",
    "\n",
    "where $\\hat{\\text{Pr}}$ is the distribution of the learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Legacy\n",
    "\n",
    "Negative legacy is unfair sampling or labeling in the training data. \n",
    "\n",
    "For example, if a bank has been refusing credit to minority people without\n",
    "assessing them, the records of minority people are less sampled in a training data\n",
    "set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Framework\n",
    "\n",
    "Given a training data set $\\cal{D} = $ $\\{(y, \\textbf{x}, s)\\}$, we can define the following terms:\n",
    "\n",
    "- $\\cal{M}$ $[ Y |X, S; \\mathbb{\\Theta}]$ conditional probability of a class given non-sensitive and sensitive features model.\n",
    "- $\\mathbb{\\Theta}$ set of model parameters. These parameters are estimates based on the maximum likelihood principle:\n",
    "$$\\cal{L}(\\cal{D}, \\mathbb{\\Theta}) = \\sum_{(y_i, \\textbf{x}_i, s_i) \\in \\cal{D}} \\ln \\cal{M} \\ [y_i|\\textbf{x}_i, s_i;\\mathbb{\\Theta}].$$\n",
    "\n",
    "For the optimization process, we use two types of regularizers, the $L_2$ regularizer $||\\mathbb{\\Theta}||_2^2$ and a second regularizer $R(\\cal{D}, \\mathbb{\\Theta})$, introduced to enforce fair classification. After applying both regularizing techniques, are objective function becomes:\n",
    "$$-\\cal{L}(\\cal{D}, \\mathbb{\\Theta}) + \\eta{} \\text{R}(\\cal{D}, \\mathbb{\\Theta}) + \\frac{\\lambda}{2} ||\\mathbb{\\Theta}||_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prejudice Remover\n",
    "\n",
    "A _prejudice remover_ regularizer directly tries to reduce the prejudice index and is denoted by $\\text{R}_{\\text{PR}}$. Recall that the prejudice index is defined as\n",
    "\n",
    "$$\\text{PI} = \\sum_{Y, S}  \\hat{\\text{Pr}}[Y, S] \\ln \\frac{\\hat{\\text{Pr}}[Y, S]}{\\hat{\\text{Pr}}[Y]\\hat{\\text{Pr}}[S]}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{\\text{Pr}}[y|s_i] \\approx \\frac{\\sum_{(\\textbf{x}_i, s_i) \\in {\\cal{D}} \\text{ s.t. } s_i = s}{\\cal{M}}[y|\\textbf{x}_i, s; \\mathbb{\\Theta}]}{|\\left\\{(\\textbf{x}_i, s_i) \\in {\\cal{D}} \\text{ s.t. } s_i = s \\right\\}|}.$$\n",
    "\n",
    "$$\\hat{\\text{Pr}}[y] \\approx \\frac{\\sum_{(\\textbf{x}_i, s_i) \\in {\\cal{D}}}{\\cal{M}}[y|\\textbf{x}_i, s_i; \\mathbb{\\Theta}]}{|{\\cal{D}}|}.$$\n",
    "\n",
    "And the prejudice remover regularizer $\\text{R}_{\\text{PR}}({\\cal{D}}, \\mathbb{\\Theta})$ is defined as\n",
    "\n",
    "$$\\sum_{(\\textbf{x}_i, s_i) \\in {\\cal{D}}}\\sum_{y\\in\\{0, 1\\}}{\\cal{M}}[y|\\textbf{x}_i, s_i;\\mathbb{\\Theta}]\\ln\\frac{\\hat{\\text{Pr}}[y|s_i]}{\\hat{\\text{Pr}}[y]}$$\n",
    "\n",
    "This regularizer becomes increasingly large as a class $y$ becomes more likely to be predicted for a sensitive group $s$ than for the entire population, thus making the overall model is influenced less by the sensitive variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "For the analysis of the algorithm, we will be using the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) dataset over a period of two years. This dataset contains information about criminal defendants and their recidivism status. The complete dataset contains 52,000 records containing and 52 features. The dataset is available on the [ProPublica GitHub](https://github.com/propublica/compas-analysis) repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This URL corresponds to the ProPublica Compas Analysis dataset \n",
    "URL = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marcu brown</td>\n",
       "      <td>marcu</td>\n",
       "      <td>brown</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-01-21</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bouthy pierrelouis</td>\n",
       "      <td>bouthy</td>\n",
       "      <td>pierrelouis</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973-01-22</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   first         last compas_screening_date   sex  \\\n",
       "id                                                                        \n",
       "1     miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
       "3          kevon dixon   kevon        dixon            2013-01-27  Male   \n",
       "4             ed philo      ed        philo            2013-04-14  Male   \n",
       "5          marcu brown   marcu        brown            2013-01-13  Male   \n",
       "6   bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
       "\n",
       "           dob  age          age_cat              race  juv_fel_count  ...  \\\n",
       "id                                                                     ...   \n",
       "1   1947-04-18   69  Greater than 45             Other              0  ...   \n",
       "3   1982-01-22   34          25 - 45  African-American              0  ...   \n",
       "4   1991-05-14   24     Less than 25  African-American              0  ...   \n",
       "5   1993-01-21   23     Less than 25  African-American              0  ...   \n",
       "6   1973-01-22   43          25 - 45             Other              0  ...   \n",
       "\n",
       "    v_decile_score  v_score_text  v_screening_date  in_custody  out_custody  \\\n",
       "id                                                                            \n",
       "1                1           Low        2013-08-14  2014-07-07   2014-07-14   \n",
       "3                1           Low        2013-01-27  2013-01-26   2013-02-05   \n",
       "4                3           Low        2013-04-14  2013-06-16   2013-06-16   \n",
       "5                6        Medium        2013-01-13         NaN          NaN   \n",
       "6                1           Low        2013-03-26         NaN          NaN   \n",
       "\n",
       "   priors_count.1 start   end event two_year_recid  \n",
       "id                                                  \n",
       "1               0     0   327     0              0  \n",
       "3               0     9   159     1              1  \n",
       "4               4     0    63     0              1  \n",
       "5               1     0  1174     0              0  \n",
       "6               2     0  1102     0              0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(URL, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary ETA and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  (7214, 52)\n",
      "name                          0\n",
      "first                         0\n",
      "last                          0\n",
      "compas_screening_date         0\n",
      "sex                           0\n",
      "dob                           0\n",
      "age                           0\n",
      "age_cat                       0\n",
      "race                          0\n",
      "juv_fel_count                 0\n",
      "decile_score                  0\n",
      "juv_misd_count                0\n",
      "juv_other_count               0\n",
      "priors_count                  0\n",
      "days_b_screening_arrest     307\n",
      "c_jail_in                   307\n",
      "c_jail_out                  307\n",
      "c_case_number                22\n",
      "c_offense_date             1159\n",
      "c_arrest_date              6077\n",
      "c_days_from_compas           22\n",
      "c_charge_degree               0\n",
      "c_charge_desc                29\n",
      "is_recid                      0\n",
      "r_case_number              3743\n",
      "r_charge_degree            3743\n",
      "r_days_from_arrest         4898\n",
      "r_offense_date             3743\n",
      "r_charge_desc              3801\n",
      "r_jail_in                  4898\n",
      "r_jail_out                 4898\n",
      "violent_recid              7214\n",
      "is_violent_recid              0\n",
      "vr_case_number             6395\n",
      "vr_charge_degree           6395\n",
      "vr_offense_date            6395\n",
      "vr_charge_desc             6395\n",
      "type_of_assessment            0\n",
      "decile_score.1                0\n",
      "score_text                    0\n",
      "screening_date                0\n",
      "v_type_of_assessment          0\n",
      "v_decile_score                0\n",
      "v_score_text                  0\n",
      "v_screening_date              0\n",
      "in_custody                  236\n",
      "out_custody                 236\n",
      "priors_count.1                0\n",
      "start                         0\n",
      "end                           0\n",
      "event                         0\n",
      "two_year_recid                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size: \", df.shape)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_records(df: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"Remove records with missing values above a threshold.\"\"\"\n",
    "    # Get the number of missing values per column\n",
    "    missing = df.isna().sum()\n",
    "    # Get the columns with missing values above the threshold\n",
    "    cols = missing[missing > threshold * df.shape[0]].index\n",
    "    # Remove the columns\n",
    "    df = df.drop(cols, axis=1)\n",
    "    # Remove the rows with missing values\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def inpute_missing_data(df):\n",
    "    \"\"\"Inpute missing data with the mean.\"\"\"\n",
    "    # Get the number of missing values per column\n",
    "    missing = df.isna().sum()\n",
    "    # Get the columns with missing values\n",
    "    cols = missing[missing > 0].index\n",
    "    # Inpute the missing values with the mean\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    return df\n",
    "\n",
    "def encode_categorical_features(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical features.\"\"\"\n",
    "    # Get the categorical features\n",
    "    categorical = df.loc[:, columns].select_dtypes(include=\"object\").columns\n",
    "    # Encode the categorical features\n",
    "    for col in categorical:\n",
    "        df[col] = pd.factorize(df[col])[0]\n",
    "    return df\n",
    "\n",
    "def parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Parse the dates.\"\"\"\n",
    "    # Convert the dates to datetime objects\n",
    "    date_columns = df.columns.str.endswith(\"_date\")\n",
    "    for col in df.loc[:, date_columns].columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess the data.\"\"\"\n",
    "    # Remove the records with missing values\n",
    "    df = remove_missing_records(df)\n",
    "    # Inpute the missing values\n",
    "    df = inpute_missing_data(df)\n",
    "    # Encode the categorical features\n",
    "    df = encode_categorical_features(df, columns)\n",
    "    # Parse the dates\n",
    "    df = parse_dates(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>0</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>0</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>0</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>marsha miles</td>\n",
       "      <td>marsha</td>\n",
       "      <td>miles</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1971-08-22</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>2013-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>edward riddle</td>\n",
       "      <td>edward</td>\n",
       "      <td>riddle</td>\n",
       "      <td>2014-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>1974-07-23</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-02-19</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   first       last compas_screening_date  sex  \\\n",
       "id                                                                   \n",
       "1   miguel hernandez  miguel  hernandez            2013-08-14    0   \n",
       "3        kevon dixon   kevon      dixon            2013-01-27    0   \n",
       "4           ed philo      ed      philo            2013-04-14    0   \n",
       "7       marsha miles  marsha      miles            2013-11-30    0   \n",
       "8      edward riddle  edward     riddle            2014-02-19    0   \n",
       "\n",
       "           dob  age  age_cat  race  juv_fel_count  ...  v_decile_score  \\\n",
       "id                                                 ...                   \n",
       "1   1947-04-18   69        0     0              0  ...               1   \n",
       "3   1982-01-22   34        1     1              0  ...               1   \n",
       "4   1991-05-14   24        2     1              0  ...               3   \n",
       "7   1971-08-22   44        1     0              0  ...               1   \n",
       "8   1974-07-23   41        1     2              0  ...               2   \n",
       "\n",
       "    v_score_text  v_screening_date  in_custody  out_custody priors_count.1  \\\n",
       "id                                                                           \n",
       "1            Low        2013-08-14  2014-07-07   2014-07-14              0   \n",
       "3            Low        2013-01-27  2013-01-26   2013-02-05              0   \n",
       "4            Low        2013-04-14  2013-06-16   2013-06-16              4   \n",
       "7            Low        2013-11-30  2013-11-30   2013-12-01              0   \n",
       "8            Low        2014-02-19  2014-03-31   2014-04-18             14   \n",
       "\n",
       "   start  end event  two_year_recid  \n",
       "id                                   \n",
       "1      0  327     0               0  \n",
       "3      9  159     1               1  \n",
       "4      0   63     0               1  \n",
       "7      1  853     0               0  \n",
       "8      5   40     1               1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"sex\", \"race\", \"age_cat\", \"c_charge_degree\", \"score_text\"]\n",
    "df = preprocess_data(df, columns)\n",
    "print(\"Missing values: \", df.isna().values.sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_feature_columns = [\n",
    "    'age', 'c_charge_degree', 'age_cat',\n",
    "    'score_text', 'sex', 'priors_count', 'days_b_screening_arrest',\n",
    "    'decile_score', 'is_recid', 'two_year_recid'\n",
    "]\n",
    "\n",
    "labels = tf.cast(df[\"two_year_recid\"],  dtype = tf.float32)\n",
    "sensitive = tf.cast(df[\"race\"],  dtype = tf.float32)\n",
    "non_sensitive = tf.cast(df[ns_feature_columns], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = tf.data.Dataset.from_tensor_slices(\n",
    "    (labels, non_sensitive, sensitive)\n",
    ")\n",
    "\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * df.shape[0])\n",
    "\n",
    "train_dataset = D.take(train_size)\n",
    "test_dataset = D.skip(train_size).take(df.shape[0] - train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(10,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_density(model, y, s, D):\n",
    "    \"\"\"Probability of label given sensitive variable.\"\"\"\n",
    "    obs = 0\n",
    "    n = 0\n",
    "    for _, xi, si in D:\n",
    "        if si == s:\n",
    "            obs += model(y, xi, s)\n",
    "            n += 1\n",
    "    if n == 0:\n",
    "        return 0.001\n",
    "\n",
    "    return obs / n\n",
    "\n",
    "def density(model, y, D):\n",
    "    \"\"\"Probability of label.\"\"\"\n",
    "    obs = 0\n",
    "    n = len(D)\n",
    "\n",
    "    for _, xi, si in D:\n",
    "        obs += model(y, xi, si)\n",
    "\n",
    "    return obs / n\n",
    "\n",
    "\n",
    "def prejudice_remover_regularizer(model, D):\n",
    "    \"\"\"Prejudice remover regularizer.\"\"\"\n",
    "    result = 0\n",
    "    for _, xi, si in D:\n",
    "        for y in [0, 1]:\n",
    "            y = tf.cast(y, dtype=tf.float32)\n",
    "            p1 = conditional_density(model, y, si, D)\n",
    "            p2 = density(model, y, D)\n",
    "            q = p1 / p2\n",
    "            log_odds = np.log(q) \n",
    "            result += log_odds * model(y, xi, si)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(BinaryLogisticRegression, self).__init__()\n",
    "        self.ws = tf.Variable(tf.random.normal(shape=(1, num_features)), name=\"w\")\n",
    "        self.b = tf.Variable(tf.zeros(shape=(1,)), name=\"b\")\n",
    "    \n",
    "    def call(self, y, x, s = 0):\n",
    "\n",
    "        x = tf.reshape(x, [-1, 1])\n",
    "        \n",
    "        # Logistic model\n",
    "        matmul = tf.matmul(x, self.ws)\n",
    "        sigmoid = tf.nn.sigmoid(matmul + self.b)\n",
    "        model_output = y * sigmoid + (1 - y) * (1 - sigmoid)\n",
    "\n",
    "        return model_output\n",
    "\n",
    "class FairnessModel(tf.keras.Model):\n",
    "    \"\"\"Fairness model.\"\"\"\n",
    "    def __init__(self, D, num_features, num_sensitive = 0):\n",
    "        super(FairnessModel, self).__init__()\n",
    "        \n",
    "        self.eta = tf.Variable(0.01)\n",
    "        self.lam = tf.Variable(0.01)\n",
    "\n",
    "        # self.model = BinaryLogisticRegression(num_features + num_sensitive)\n",
    "        self.model = BinaryLogisticRegression(num_features)\n",
    "        self.D = D  \n",
    "        self.R_pr = prejudice_remover_regularizer\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, D):\n",
    "        \"\"\"Call the model.\"\"\"\n",
    "        for y, x, s in D:\n",
    "            # Compute the model output\n",
    "            model_output = self.model(y, x)\n",
    "\n",
    "        # Prejudice remover regularizer\n",
    "        R_pr = self.R_pr(self.model, self.D)\n",
    "        regularizer = self.eta * R_pr\n",
    "\n",
    "        # l2 regularization\n",
    "        l2 = self.lam * tf.reduce_sum(tf.square(self.model.ws))\n",
    "\n",
    "        log_model_output = tf.math.log(model_output)\n",
    "        output = tf.reduce_sum(log_model_output) + regularizer + l2\n",
    "\n",
    "        return output\n",
    "\n",
    "    def fit(self, D, epochs=100):\n",
    "        \"\"\"Fit the model.\"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "        for _ in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self.call(D)\n",
    "            grads = tape.gradient(loss, self.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [118], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m FairnessModel(train_dataset, non_sensitive\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [116], line 57\u001b[0m, in \u001b[0;36mFairnessModel.fit\u001b[1;34m(self, D, epochs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     56\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> 57\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(D)\n\u001b[0;32m     58\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn [116], line 38\u001b[0m, in \u001b[0;36mFairnessModel.call\u001b[1;34m(self, D)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m\"\"\"Call the model.\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m y, x, s \u001b[39min\u001b[39;00m D:\n\u001b[0;32m     37\u001b[0m     \u001b[39m# Compute the model output\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(y, x)\n\u001b[0;32m     40\u001b[0m \u001b[39m# Prejudice remover regularizer\u001b[39;00m\n\u001b[0;32m     41\u001b[0m R_pr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR_pr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [116], line 14\u001b[0m, in \u001b[0;36mBinaryLogisticRegression.call\u001b[1;34m(self, y, x, s)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Logistic model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m matmul \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mws)\n\u001b[1;32m---> 14\u001b[0m sigmoid \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msigmoid(matmul \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb)\n\u001b[0;32m     15\u001b[0m model_output \u001b[39m=\u001b[39m y \u001b[39m*\u001b[39m sigmoid \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m sigmoid)\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m model_output\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1407\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1403\u001b[0m   \u001b[39m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1406\u001b[0m   x, y \u001b[39m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1407\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   1408\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1409\u001b[0m   \u001b[39m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m   \u001b[39m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m   \u001b[39m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m   \u001b[39m# informative.\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mtype\u001b[39m(y), \u001b[39m\"\u001b[39m\u001b[39m__r\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m op_name):\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1757\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1755\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39madd(x, y, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   1756\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49madd_v2(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\alixl\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:461\u001b[0m, in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    460\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    462\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mAddV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    464\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FairnessModel(train_dataset, non_sensitive.shape[1])\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, test_labels, test_imgs):\n",
    "    y_true = test_labels\n",
    "    y_pred = []\n",
    "    for image in test_imgs:\n",
    "        y_pred.append(model(image))\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b68576675d14688f13df6495c027427b1fa86cc0b514974591414d368704a84c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
